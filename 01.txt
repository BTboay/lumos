0.020030  0.020030  0.010035  0.020035  0.030035  0.020035  0.020025  0.010025
0.001060  0.001070  0.001070  0.001050

0.050998  0.020998  0.030998  0.030998  0.010998  0.050998  0.040998  0.020998
0.001998  0.001998

0.109786  0.109786
0.100786

batch: 4  subdivision: 2

00
01
11
10

4*2  2*1 -->  4*1
2*4  4*1 -->  2*1
1*2  2*1 -->  1*1

Forward
------------------------------------------------------------------------------------------------
layer 1:(connect)
weights               inputs    gemm(NN)    bias        outputs
0.020030  0.020030    0.0       0.0         0.001060    0.001060
0.010035  0.020035    0.0       0.0         0.001070    0.001070
0.030035  0.020035              0.0         0.001070    0.001070
0.020025  0.010025              0.0         0.001050    0.001050

layer 2:(connect)
weights                                 inputs      gemm(NN)            bias         outputs
0.050998  0.020998  0.030998  0.030998  0.001060    0.0001422415        0.001998     0.0021402415
0.010998  0.050998  0.040998  0.020998  0.001070    0.0001321415        0.001998     0.0021301415
                                        0.001070
                                        0.001050

layer 3:(connect)
weights                 inputs          gemm(NN)            bias        outputs
0.109786  0.109786      0.0021402415    0.0004688283        0.100786    0.1012548283
                        0.0021301415

layer 4:(mse)
outputs
0.010252540254062482
------------------------------------------------------------------------------------------------

------------------------------------------------------------------------------------------------
layer 1:(connect)
weights               inputs    gemm(NN)        bias        outputs
0.020030  0.020030    0.0       0.020030        0.001060    0.021090
0.010035  0.020035    1.0       0.020035        0.001070    0.021105
0.030035  0.020035              0.020035        0.001070    0.021105
0.020025  0.010025              0.010025        0.001050    0.011075

layer 2:(connect)
weights                                 inputs      gemm(NN)            bias        outputs
0.050998  0.020998  0.030998  0.030998  0.021090    0.00251622625       0.001998    0.00451422625
0.010998  0.050998  0.040998  0.020998  0.021105    0.00240607625       0.001998    0.00440407625
                                        0.021105
                                        0.011075

layer 3:(connect)
weights                 inputs           gemm(NN)               bias                  outputs
0.109786  0.109786      0.00451422625    0.000979104758265      0.101765104758265     0.101765104758265
                        0.00440407625

layer 4:(mse)
outputs
0.8068259270299305
------------------------------------------------------------------------------------------------

------------------------------------------------------------------------------------------------
final result:
layer 1:(connect)
0.001060  0.001070  0.001070  0.001050  0.021090  0.021105  0.021105  0.011075

layer 2:(connect)
0.0021402415  0.0021301415  0.00451422625  0.00440407625

layer 3:(connect)
0.1012548283  0.101765104758265

layer 4:(mse)
0.010252540254062482  0.8068259270299305

loss=0.4085392336419965
------------------------------------------------------------------------------------------------

Backward
------------------------------------------------------------------------------------------------
layer 4:(mse)
0.2025096566

layer 3:(connect)
weights                 delta_n        gemm(TN)               delta_l
0.109786  0.109786      0.2025096566   0.0222327251594876     0.0222327251594876
                                       0.0222327251594876     0.0222327251594876

layer 2:(connect)
weights                                 delta_n                gemm(TN)
0.050998  0.020998  0.030998  0.030998  0.0222327251594876     0.0013783400289875932
0.010998  0.050998  0.040998  0.020998  0.0222327251594876     0.0016006672805824692
                                                               0.0016006672805824692
                                                               0.0011560127773927171

layer 1:(connect)
weights               delta_n                     gemm(TN)
0.020030  0.020030    0.0013783400289875932       0.0001148960445808502
0.010035  0.020035    0.0016006672805824692       0.00010333591680692302
0.030035  0.020035    0.0016006672805824692
0.020025  0.010025    0.0011560127773927171
------------------------------------------------------------------------------------------------

------------------------------------------------------------------------------------------------
layer 4:(mse)
-1.7964697904834699

layer 3:(connect)
weights                 delta_n                 gemm(TN)               delta_l
0.109786  0.109786      -1.7964697904834699     -0.197227232418018     -0.197227232418018
                                                -0.197227232418018     -0.197227232418018

layer 2:(connect)
weights                                 delta_n                gemm(TN)
0.050998  0.020998  0.030998  0.030998  -0.197227232418018     -0.012227299500987445
0.010998  0.050998  0.040998  0.020998  -0.197227232418018     -0.014199571825167626
                                                               -0.014199571825167626
                                                               -0.010255027176807265